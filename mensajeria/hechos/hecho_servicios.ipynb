{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "498e6b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sqlalchemy in e:\\univalle\\13. séptimo semestre (repetición)\\introd ciencias de los datos\\clase 8\\etl\\venv\\lib\\site-packages (2.0.41)\n",
      "Requirement already satisfied: greenlet>=1 in e:\\univalle\\13. séptimo semestre (repetición)\\introd ciencias de los datos\\clase 8\\etl\\venv\\lib\\site-packages (from sqlalchemy) (3.2.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in e:\\univalle\\13. séptimo semestre (repetición)\\introd ciencias de los datos\\clase 8\\etl\\venv\\lib\\site-packages (from sqlalchemy) (4.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: psycopg2 in e:\\univalle\\13. séptimo semestre (repetición)\\introd ciencias de los datos\\clase 8\\etl\\venv\\lib\\site-packages (2.9.10)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: psycopg2-binary in e:\\univalle\\13. séptimo semestre (repetición)\\introd ciencias de los datos\\clase 8\\etl\\venv\\lib\\site-packages (2.9.10)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in e:\\univalle\\13. séptimo semestre (repetición)\\introd ciencias de los datos\\clase 8\\etl\\venv\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in e:\\univalle\\13. séptimo semestre (repetición)\\introd ciencias de los datos\\clase 8\\etl\\venv\\lib\\site-packages (from scikit-learn) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.8.0 in e:\\univalle\\13. séptimo semestre (repetición)\\introd ciencias de los datos\\clase 8\\etl\\venv\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in e:\\univalle\\13. séptimo semestre (repetición)\\introd ciencias de los datos\\clase 8\\etl\\venv\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in e:\\univalle\\13. séptimo semestre (repetición)\\introd ciencias de los datos\\clase 8\\etl\\venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in e:\\univalle\\13. séptimo semestre (repetición)\\introd ciencias de los datos\\clase 8\\etl\\venv\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in e:\\univalle\\13. séptimo semestre (repetición)\\introd ciencias de los datos\\clase 8\\etl\\venv\\lib\\site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\univalle\\13. séptimo semestre (repetición)\\introd ciencias de los datos\\clase 8\\etl\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\univalle\\13. séptimo semestre (repetición)\\introd ciencias de los datos\\clase 8\\etl\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\univalle\\13. séptimo semestre (repetición)\\introd ciencias de los datos\\clase 8\\etl\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in e:\\univalle\\13. séptimo semestre (repetición)\\introd ciencias de los datos\\clase 8\\etl\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Instalación de librerías requeridas\n",
    "%pip install sqlalchemy\n",
    "%pip install psycopg2\n",
    "%pip install psycopg2-binary\n",
    "%pip install scikit-learn\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad2e711d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "from sqlalchemy import create_engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53ca9ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'driver': 'postgresql+psycopg2',\n",
       " 'host': 'proyectobodega.postgres.database.azure.com',\n",
       " 'port': 5432,\n",
       " 'user': 'adminbodega',\n",
       " 'password': 'Goddess9039#',\n",
       " 'db': 'proyectobodega'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../../configBD/config.yml', 'r') as f:\n",
    "    cfg        = yaml.safe_load(f)\n",
    "    cfg_etl    = cfg['bodega']\n",
    "    cfg_bd     = cfg['mensajeria']\n",
    "cfg_etl       # verificación rápida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feff970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_bd  = f\"{cfg_bd['driver']}://{cfg_bd['user']}:{cfg_bd['password']}@{cfg_bd['host']}:{cfg_bd['port']}/{cfg_bd['db']}\"\n",
    "url_etl = f\"{cfg_etl['driver']}://{cfg_etl['user']}:{cfg_etl['password']}@{cfg_etl['host']}:{cfg_etl['port']}/{cfg_etl['db']}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "798de0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cliente_bd  = create_engine(url_bd)     # base operativa\n",
    "cliente_etl = create_engine(url_etl)    # Data Warehouse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ffd81c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_cliente   = pd.read_sql_table('dim_cliente',   url_etl)\n",
    "dim_mensajero = pd.read_sql_table('dim_mensajero', url_etl)\n",
    "dim_sede      = pd.read_sql_table('dim_sede',      url_etl)\n",
    "dim_tiempo    = pd.read_sql_table('dim_tiempo',    url_etl)\n",
    "\n",
    "# normaliza dim_tiempo → genera columna date\n",
    "dim_tiempo = dim_tiempo.rename(columns={'Año':'year','Mes':'month','Dia':'day'})\n",
    "dim_tiempo['date'] = pd.to_datetime(dim_tiempo[['year','month','day']]).dt.date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f187b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "servicios      = pd.read_sql_table('mensajeria_servicio',        url_bd)\n",
    "origenes       = pd.read_sql_table('mensajeria_origenservicio',  url_bd)\n",
    "destinos       = pd.read_sql_table('mensajeria_destinoservicio', url_bd)\n",
    "estados_srv    = pd.read_sql_table('mensajeria_estadosservicio', url_bd)\n",
    "cat_estado     = pd.read_sql_table('mensajeria_estado',          url_bd)\n",
    "cat_tipo_srv   = pd.read_sql_table('mensajeria_tiposervicio',    url_bd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28d24f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'descripcion', 'nombre_solicitante', 'fecha_solicitud', 'hora_solicitud', 'fecha_deseada', 'hora_deseada', 'nombre_recibe', 'telefono_recibe', 'descripcion_pago', 'ida_y_regreso', 'activo', 'novedades', 'cliente_id', 'destino_id', 'mensajero_id', 'origen_id', 'tipo_pago_id', 'tipo_servicio_id', 'tipo_vehiculo_id', 'usuario_id', 'prioridad', 'ciudad_destino_id', 'ciudad_origen_id', 'hora_visto_por_mensajero', 'visto_por_mensajero', 'descripcion_multiples_origenes', 'mensajero2_id', 'mensajero3_id', 'multiples_origenes', 'asignar_mensajero', 'es_prueba', 'descripcion_cancelado', 'FechaSolicitud', 'TiempoKey']\n",
      "['id', 'descripcion', 'nombre_solicitante', 'fecha_solicitud', 'hora_solicitud', 'fecha_deseada', 'hora_deseada', 'nombre_recibe', 'telefono_recibe', 'descripcion_pago', 'ida_y_regreso', 'activo', 'novedades', 'cliente_id', 'destino_id', 'mensajero_id', 'origen_id', 'tipo_pago_id', 'tipo_servicio_id', 'tipo_vehiculo_id', 'usuario_id', 'prioridad', 'ciudad_destino_id', 'ciudad_origen_id', 'hora_visto_por_mensajero', 'visto_por_mensajero', 'descripcion_multiples_origenes', 'mensajero2_id', 'mensajero3_id', 'multiples_origenes', 'asignar_mensajero', 'es_prueba', 'descripcion_cancelado', 'FechaSolicitud', 'TiempoKey', 'ClienteKey']\n",
      "['id', 'descripcion', 'nombre_solicitante', 'fecha_solicitud', 'hora_solicitud', 'fecha_deseada', 'hora_deseada', 'nombre_recibe', 'telefono_recibe', 'descripcion_pago', 'ida_y_regreso', 'activo', 'novedades', 'cliente_id', 'destino_id', 'mensajero_id', 'origen_id', 'tipo_pago_id', 'tipo_servicio_id', 'tipo_vehiculo_id', 'usuario_id', 'prioridad', 'ciudad_destino_id', 'ciudad_origen_id', 'hora_visto_por_mensajero', 'visto_por_mensajero', 'descripcion_multiples_origenes', 'mensajero2_id', 'mensajero3_id', 'multiples_origenes', 'asignar_mensajero', 'es_prueba', 'descripcion_cancelado', 'FechaSolicitud', 'TiempoKey', 'ClienteKey', 'MensajeroKey']\n"
     ]
    }
   ],
   "source": [
    "# ----- 0) copia base -----\n",
    "fact_serv = servicios.copy()\n",
    "\n",
    "# ----- 1) TiempoKey (fecha_solicitud) -----\n",
    "fact_serv['FechaSolicitud']   = pd.to_datetime(fact_serv['fecha_solicitud']).dt.date\n",
    "dim_tiempo_subset = dim_tiempo[['tiempo_key','date']].rename(columns={'tiempo_key':'TiempoKey'})\n",
    "fact_serv = fact_serv.merge(dim_tiempo_subset,\n",
    "                            left_on='FechaSolicitud',\n",
    "                            right_on='date', how='left') \\\n",
    "                     .drop(columns=['date'])\n",
    "print(fact_serv.columns.tolist())\n",
    "\n",
    "\n",
    "# ----- 2) FK Cliente -----\n",
    "fact_serv = fact_serv.merge(dim_cliente[['ClienteKey','cliente_id']],\n",
    "                            on='cliente_id', how='left')\n",
    "print(fact_serv.columns.tolist())\n",
    "\n",
    "\n",
    "# ----- 3) FK Mensajero (titular) -----\n",
    "fact_serv = fact_serv.merge(dim_mensajero[['MensajeroKey','user_id']],\n",
    "                            left_on='mensajero_id', right_on='user_id',\n",
    "                            how='left').drop(columns=['user_id'])\n",
    "print(fact_serv.columns.tolist())\n",
    "\n",
    "\n",
    "# ----- 4) FK Sede Origen y Destino -----\n",
    "# 4a) unir origen\n",
    "fact_serv = (\n",
    "    fact_serv\n",
    "    .merge(\n",
    "        origenes[['id','cliente_id','ciudad_id']],\n",
    "        left_on='origen_id', right_on='id',\n",
    "        how='left',\n",
    "        suffixes=('','_ori')      # id→id_ori\n",
    "    )\n",
    "    .rename(columns={\n",
    "        'ciudad_id':      'ciudad_ori',\n",
    "        'cliente_id_ori': 'cliente_ori'\n",
    "    })\n",
    "    .drop(columns=['id_ori'])    # <— borramos sólo id_ori, NO id\n",
    ")\n",
    "\n",
    "# 4b) unir destino\n",
    "fact_serv = (\n",
    "    fact_serv\n",
    "    .merge(\n",
    "        destinos[['id','cliente_id','ciudad_id']],\n",
    "        left_on='destino_id', right_on='id',\n",
    "        how='left',\n",
    "        suffixes=('','_des')      # id→id_des\n",
    "    )\n",
    "    .rename(columns={\n",
    "        'ciudad_id':      'ciudad_des',\n",
    "        'cliente_id_des': 'cliente_des'\n",
    "    })\n",
    "    .drop(columns=['id_des'])    # <— borramos sólo id_des\n",
    ")\n",
    "\n",
    "\n",
    "# 4c  lookup en dim_sede\n",
    "# fact_serv = fact_serv.merge(dim_sede[['SedeKey','ciudad_id','cliente_id']],\n",
    "#                             left_on=['ciudad_ori','cliente_ori'],\n",
    "#                             right_on=['ciudad_id','cliente_id'],\n",
    "#                             how='left').rename(columns={'SedeKey':'SedeOrigenKey'}) \\\n",
    "#                      .drop(columns=['ciudad_id','cliente_id'])\n",
    "# 4c) lookup en dim_sede para SedeOrigenKey\n",
    "fact_serv = (\n",
    "    fact_serv\n",
    "    .merge(\n",
    "        dim_sede[['SedeKey','ciudad_id','cliente_id']],\n",
    "        left_on=['ciudad_ori','cliente_ori'],\n",
    "        right_on=['ciudad_id','cliente_id'],\n",
    "        how='left',\n",
    "        suffixes=('','_dim')           # sufijo solo en cliente_id de dim_sede\n",
    "    )\n",
    "    .rename(columns={'SedeKey':'SedeOrigenKey'})\n",
    "    .drop(columns=['ciudad_id','cliente_id_dim'])  # elimina la ciudad_id traída y el cliente_id_dim\n",
    ")\n",
    "\n",
    "# 4d) lookup en dim_sede para SedeDestinoKey (idéntico, cambiando sólo nombres)\n",
    "fact_serv = (\n",
    "    fact_serv\n",
    "    .merge(\n",
    "        dim_sede[['SedeKey','ciudad_id','cliente_id']],\n",
    "        left_on=['ciudad_des','cliente_des'],\n",
    "        right_on=['ciudad_id','cliente_id'],\n",
    "        how='left',\n",
    "        suffixes=('','_dim')\n",
    "    )\n",
    "    .rename(columns={'SedeKey':'SedeDestinoKey'})\n",
    "    .drop(columns=['ciudad_id','cliente_id_dim'])\n",
    ")\n",
    "\n",
    "# ----- 5) TipoServicio -----\n",
    "fact_serv = (\n",
    "    fact_serv\n",
    "    .merge(\n",
    "        cat_tipo_srv[['id','nombre']],\n",
    "        left_on='tipo_servicio_id',\n",
    "        right_on='id',\n",
    "        how='left',\n",
    "        suffixes=('','_tipo')\n",
    "    )\n",
    "    .rename(columns={'nombre':'TipoServicio'})\n",
    "    .drop(columns=['id_tipo'])\n",
    ")\n",
    "\n",
    "# ----- 6) EstadoServicio (último estado registrado) -----\n",
    "# 6a) fusionar nombre del estado\n",
    "# 6a) fusionar nombre del estado\n",
    "estados_srv = (\n",
    "    estados_srv\n",
    "    .merge(\n",
    "        cat_estado[['id','nombre']],\n",
    "        left_on='estado_id',\n",
    "        right_on='id',\n",
    "        how='left',\n",
    "        suffixes=('','_cat')\n",
    "    )\n",
    "    .rename(columns={'nombre':'EstadoNom'})\n",
    "    .drop(columns=['id_cat'])\n",
    ")\n",
    "\n",
    "# ———————> dedupe columns <———————\n",
    "estados_srv = estados_srv.loc[:, ~estados_srv.columns.duplicated()]\n",
    "\n",
    "\n",
    "# 6b) timestamp con microsegundos\n",
    "estados_srv['timestamp'] = pd.to_datetime(\n",
    "    estados_srv['fecha'].astype(str) + ' ' + estados_srv['hora'].astype(str),\n",
    "    format='%Y-%m-%d %H:%M:%S.%f',\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# 6c) extraer último estado\n",
    "ult_estado = (\n",
    "    estados_srv\n",
    "    .sort_values('timestamp')\n",
    "    .groupby('servicio_id', as_index=False)\n",
    "    .tail(1)[['servicio_id','EstadoNom','timestamp']]\n",
    ")\n",
    "\n",
    "# 6d) merge en fact_serv\n",
    "fact_serv = (\n",
    "    fact_serv\n",
    "    .merge(ult_estado,\n",
    "           left_on='id', right_on='servicio_id',\n",
    "           how='left')\n",
    "    .rename(columns={'EstadoNom':'EstadoServicio'})\n",
    "    .drop(columns=['servicio_id'])\n",
    ")\n",
    "\n",
    "# ————————————— CORRECCIÓN —————————————\n",
    "# Elimina cualquier columna duplicada (por si quedaron dos 'EstadoServicio')\n",
    "fact_serv = fact_serv.loc[:, ~fact_serv.columns.duplicated()]\n",
    "\n",
    "# ----- 7) EsFinal (TRUE si el estado es uno de los finales) -----\n",
    "finales = ['ENTREGADO','CERRADO','CANCELADO']\n",
    "fact_serv['EsFinal'] = fact_serv['EstadoServicio'].isin(finales)\n",
    "\n",
    "# ----- 8) Timestamps de tracking -----\n",
    "fact_serv['Tiempo_Inicio'] = pd.to_datetime(\n",
    "    fact_serv['fecha_solicitud'].astype(str) + ' ' + fact_serv['hora_solicitud'].astype(str),\n",
    "    errors='coerce'\n",
    ")\n",
    "fact_serv['Tiempo_Mensajero_Asignado'] = pd.to_datetime(\n",
    "    fact_serv['hora_visto_por_mensajero'], errors='coerce'\n",
    ")\n",
    "\n",
    "def _extrae_evt(df, estado_nombre, new_col):\n",
    "    tmp = df[df['EstadoNom'] == estado_nombre].copy()\n",
    "    tmp['ts'] = pd.to_datetime(\n",
    "        tmp['fecha'].astype(str) + ' ' + tmp['hora'].astype(str),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    return (\n",
    "        tmp.sort_values('ts')\n",
    "           .groupby('servicio_id', as_index=False)\n",
    "           .head(1)[['servicio_id','ts']]\n",
    "           .rename(columns={'ts': new_col})\n",
    "    )\n",
    "\n",
    "for estado, new_col in [\n",
    "    ('RECOGIDO_ORIGEN',     'Tiempo_Recogido_Origen'),\n",
    "    ('ENTREGADO_DESTINO',   'Tiempo_Entregado_Destino'),\n",
    "    ('CERRADO',             'Tiempo_Cerrado')\n",
    "]:\n",
    "    evt = _extrae_evt(estados_srv, estado, new_col)\n",
    "    fact_serv = (\n",
    "        fact_serv\n",
    "        .merge(evt,\n",
    "               left_on='id',\n",
    "               right_on='servicio_id',\n",
    "               how='left')\n",
    "        .drop(columns=['servicio_id'])\n",
    "    )\n",
    "\n",
    "\n",
    "# ----- 9) limpieza de columnas intermedias -----\n",
    "drop_cols = (\n",
    "    [c for c in fact_serv.columns if c.startswith(('fecha_','hora_','ciudad_','cliente_'))]\n",
    "    + ['servicio_id']\n",
    ")\n",
    "fact_serv = fact_serv.drop(columns=drop_cols, errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6d7dbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ServicioKey  TiempoKey  ClienteKey  SedeOrigenKey  SedeDestinoKey  \\\n",
      "0           34     7152.0           7           16.0            16.0   \n",
      "1           34     7152.0           7           16.0            17.0   \n",
      "2           34     7152.0           7           16.0            19.0   \n",
      "3           34     7152.0           7           16.0            24.0   \n",
      "4           34     7152.0           7           16.0            49.0   \n",
      "\n",
      "   MensajeroKey TipoServicio EstadoServicio  EsFinal       Tiempo_Inicio  \\\n",
      "0           NaN      Clínico       Iniciado    False 2023-10-26 09:46:03   \n",
      "1           NaN      Clínico       Iniciado    False 2023-10-26 09:46:03   \n",
      "2           NaN      Clínico       Iniciado    False 2023-10-26 09:46:03   \n",
      "3           NaN      Clínico       Iniciado    False 2023-10-26 09:46:03   \n",
      "4           NaN      Clínico       Iniciado    False 2023-10-26 09:46:03   \n",
      "\n",
      "  Tiempo_Mensajero_Asignado Tiempo_Recogido_Origen Tiempo_Entregado_Destino  \\\n",
      "0                       NaT                    NaT                      NaT   \n",
      "1                       NaT                    NaT                      NaT   \n",
      "2                       NaT                    NaT                      NaT   \n",
      "3                       NaT                    NaT                      NaT   \n",
      "4                       NaT                    NaT                      NaT   \n",
      "\n",
      "  Tiempo_Cerrado  \n",
      "0            NaT  \n",
      "1            NaT  \n",
      "2            NaT  \n",
      "3            NaT  \n",
      "4            NaT  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # 0) “Subconjunto” de las primeras 1 000 filas\n",
    "# fact_serv_sub = fact_serv.head(1000)      # o fact_serv.iloc[:1000]\n",
    "# print(fact_serv_sub.shape)                # debe mostrar (1000, n_columnas)\n",
    "# print(fact_serv_sub.head())               # vistazo a las primeras filas\n",
    "\n",
    "# # 1a) Limpia la transacción pendiente\n",
    "# raw = cliente_etl.raw_connection()\n",
    "# raw.rollback()\n",
    "# raw.close()\n",
    "\n",
    "# # 1b) Vuelca solo esas 1 000 filas con el mismo método\n",
    "# fact_serv_sub.to_sql(\n",
    "#     'fact_servicios',\n",
    "#     cliente_etl,\n",
    "#     if_exists='replace',\n",
    "#     index=False,\n",
    "#     method='multi',\n",
    "#     chunksize=1000\n",
    "# )\n",
    "\n",
    "# 0) renombra el PK para que sea ServicioKey\n",
    "fact_serv = fact_serv.rename(columns={'id':'ServicioKey'})\n",
    "\n",
    "# 1) define el orden definitivo\n",
    "cols_final = [\n",
    "    'ServicioKey','TiempoKey','ClienteKey','SedeOrigenKey','SedeDestinoKey',\n",
    "    'MensajeroKey','TipoServicio','EstadoServicio','EsFinal',\n",
    "    'Tiempo_Inicio','Tiempo_Mensajero_Asignado',\n",
    "    'Tiempo_Recogido_Origen','Tiempo_Entregado_Destino','Tiempo_Cerrado'\n",
    "]\n",
    "\n",
    "# 2) filtra solo esas columnas\n",
    "fact_serv = fact_serv[cols_final]\n",
    "\n",
    "# 3) muestra las primeras filas\n",
    "print(fact_serv.head())\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
